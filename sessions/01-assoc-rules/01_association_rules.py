# -*- coding: utf-8 -*-
"""01-association-rules

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WBeIpWXQUVqhAEPTadDnX9FV3uJCabZu
"""

# installs
# pip install mlxtend

# imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px

from mlxtend.frequent_patterns import apriori, association_rules

# my project in Google Cloud used
PROJECT = "questrom"

# auth against your BU GCP account 

# for colab
# from google.colab import auth
# auth.authenticate_user()
# print('Authenticated')

# EXERCISE: get the data from Big Query
# select all the records from 
# `questrom.datasets.groceries`

# quick look

# first few rows

"""#### Data Dictionary

- tid = transaction id
- item = the product
"""

## QUICK EXERCISE:
## take 3 minutes, how many unique transactions 
## and items?
## are there any duplicates?

### its worth noting that I tend to like this tidy format
### its easy to filter on a transaction and clearly see the detail
### it also works great when we have an element of time/sequence in our datasets

## put the data into transaction format
## note that mlxtend wants a 1 row per transaction and 1-hot layout

# when we pivot, the intersections will be True
# groceries['purchase'] = True

# tx = groceries.pivot(index="tid", columns="item", values="purchase")

# what do we have



# we need to fill in the missings, which is easy with pandas
# tx.fillna(value=False, inplace=True)
# tx.head(3)

# plot the items per transaction
# item_count = tx.sum(axis=1)

# print(item_count.shape)

# item_count.value_counts(ascending=False).plot(kind="bar")

## why does this help us understand the dataset?

# understanding how often a product exists will help us with pruning
# item_freq = tx.sum(axis=0)

# print(item_freq.shape)

# # normalize against all transactions
# item_freq = item_freq / len(tx)

# # plot
# sns.histplot(item_freq)

# summarize the distro with describe

## what does this tell us about suppport settings?

## QUICK EXERCISE:  What are the top 5 products?

# apply the apriori algorithm to the dataset
# itemsets = apriori(tx, min_support=.003, use_colnames=True)

# what do we have?



## lets fit our first assoc rules model!
## http://rasbt.github.io/mlxtend/api_subpackages/mlxtend.frequent_patterns/

## leaving confidence low to demonstrate rule evaluation

# what do we have

# first few

# mlxtend uses frozensets, whichm are sets but immutable

# lets look for the RHS that hsa bottled water

# rules.loc[rules.consequents == {'bottled water'}, :].head()

## Exercise:  5 minutes, think about the following questions
##   how many rules were created
##   what is the range of lift values
##   what is the average rule size (how many items)
##      HINT:  lambdas 
##   plot (barplot) of rule sizes

## barplot of rule sizes

# lets look at the first few rows

## EXERCISE:
## calculate the support for the rule below
## HINT:  you can use the prior datasets
## The Rule: Instant food products -> hamburger meat

# of course, because we have a pandas dataframe, its very simple to inspect

## Exercise - Take 5 minutes
## 1.  sort the rules ascending by lift - print out the first 5
## 2.  find the top 10 rules (sorted descending by support) where chewing gum
##     is in the RHS

## 1



## 2

## QUICK QUESTION:  From an analyst point of view, why might we want to 
##                  filter the rules from the start?  Example applications of
##                  where we might do this?

## Visualize the rules 
## support and lift
## as DC noted, this can help you with filtering if you start relatively unpruned

# px.scatter(rules, x="antecedent support", y="consequent support", size="lift")

## we could also look at support/confidence/lift

# plt.figure(figsize=(10,4))
# sns.scatterplot(data=rules, x="support", y="confidence", hue="lift", alpha=.7)