# -*- coding: utf-8 -*-
"""10 - spacy - NER - Vectors - Classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AYfuceyUc5fyivjdG1BDIf4PD0T_PVTz
"""

##############################################################################
## Fundamentals for pratical Text Analytics - spacy for language modeling, NER, 
##                                          roll our own intent sclassification 
##
## Learning goals:
##                 - reinforce text as a robust dataset via language modeling
##                 - python packages for handling our corpus for these specific tasks
##                 - SPACY!
##                 - POS tagging (to help with extraction/classification)
##                 - NER extraction
##                 - generalized, pre-trained word vectors for S|UML tasks (intent classification)
##############################################################################

# installs
! pip install newspaper3k
! pip install spacy
! pip install wordcloud
! pip install emoji
! pip install nltk
! pip install scikit-plot
! pip install umap-learn
! pip install afinn
! pip install textblob
! pip install gensim
! pip install pysrt
! pip install wikipedia

# imports
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import scikitplot as skplot

# some "fun" packages
from wordcloud import WordCloud
import emoji

import re

# text imports
import spacy
from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')

import gensim

from afinn import Afinn

from newspaper import Article

##################################### WARMUP Exercise
##################################### ~ 10
## there is a file located below: 
##     https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt
## there is also some started code below
## calculate the sentiment over the course of the movie script (Shrek)
## plot the sentiment arc over the movie
##

# 0. get the file - the file in the browser will auto download
#    just make sure you have the file in your working directory

# or if on colab
! wget https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt

# 1. get the file and parse
import pysrt
subs = pysrt.open('Shrek-2001.srt', encoding='iso-8859-1')

##################################### Quick example 
##################################### Key Words in Context - Concordance
###
### powerful tool to look at a set of text (full corpus) and look for
### words before/after
###
### helpful for eda, look for patterns to help support data annotation, etc.


# just in case
# nltk.download('punkt')

# get the data
# SQL = "SELECT * FROM `questrom.datasets.topics`"
# PROJ = "questrom"
# intents = pd.read_gbq(SQL, PROJ)

# text
# from nltk import Text

# get the text into the correct format
# intents_text = intents.text.tolist()
# print(len(intents_text))

# put into a corpus (as if it were 1 big file, ignoring that we have intents)
# corpus = " ".join(intents_text)

# tokenize
# tokens = nltk.word_tokenize(corpus)
# len(tokens)

# put the corpus into a Text object.  
# some nice features when we consider the corpus a blob of text that lacks 
# structure like sentences, or by user, etc.  Just the text combined like a book chapter.
# text = Text(tokens)

# look for the context of words
# Key work in context, or concordance

## shipping / help  / size / discount
# text.concordance("product", width=80, lines=10)

# above is referred to as key words in context via quanteda in R

##################################### REFERENCE AND THOUGHT MATERIAL: Emojis
## 
##
## get the emoji sentiment database here: http://kt.ijs.si/data/Emoji_sentiment_ranking/ 

## HINT:  pandas can parse web tables, remember! via read_html, and then we have a list of dataframes
## 
## 
## sources of the datasets (check out the about)
## real source is: https://www.clarin.si/repository/xmlui/handle/11356/1048
##



######################################### lets get started with spacy!
##
## "industrial strength" NLP tools
## increasingly the go to in this space, but like Textblob, 
## it focuses on document-oriented operations
## lots of configuration and power, but lets get the foundations set
##



# setup spacy - I am using CLI below because I think its easier and works on colab too
# you can see this done via something like below
# 
# from spacy.lang.en import English
# nlp = English()
# 

# If VS Code crashes
# use
# (from command line python -m spacy download en_core_web_md)
# and ignore the cli commands below, proceed to spacy.load

# import spacy.cli 

# https://spacy.io/models/en
# download the medium model - not the sm model
# spacy.cli.download("en_core_web_md")


# spacy operates via language models
# https://spacy.io/models/en
# we are going to setup the model as nlp, the spacy convention

# this will take a few moments
# most examples start with small, but we are going to use a larger model
# nlp = spacy.load('en_core_web_md')

# the message we will parse into a document


# we are using all of the defaults, but this is a "heavy" parse with a ton of great data to explore
# https://spacy.io/usage/spacy-101#features

# what do we have?

# notice that we have tokens and from there a document
#
# as we have, check out what we can do
# doc tab, or shift tab

# doc.
# or you can use dir(doc)

# length

# we can list it of course

# but this is so much more than a simple string!
# 
# the tokens have properties/attributes
# https://spacy.io/usage/linguistic-features#tokenization

# also, when tokenized, all sorts of atributes
# quick example to get rid of the punct as just one example of inspection

# tokens = [token.text for token in doc if not token.is_punct]
# tokens

## of course there is so much you can do here
## 
## but lets just focus on spacy suggesting what they believe are best practices by default (keep punct)
## 
## just like we have seen, we can use the language model to parse tokens, and 
## pass that to sklearn, but that is wildly under-utilizing spacy
##
## But spacy has all sorts of great features, lets explore!
##

# lets see what this NLP model is all above
# parsed = []
# for token in doc:
#   parsed.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_))

# parsed

# ok, I am the farthest thing from a linguist, 
# https://spacy.io/usage/linguistic-features#pos-tagging
#
# during lemma, this might help: 
# https://stackoverflow.com/questions/50543752/spacy-lemmatization-on-pronouns-gives-some-erronous-output

# luckily we can use explain

# spacy, via it's models (learned from labeled data!) can even map the relationships
# from spacy import displacy

# for those in jupyter or colab
# displacy.render(doc, style="dep", jupyter=True)

# if in vs code - spacy has a built in server
# the viz is at http://localhost:5000 
# displacy.serve(doc, style="dep")

#######################################  YOUR TURN
# using the corpus below, parse the text and POS tag, and lemma

corpus = ["Brock likes hockey",
          "I love teaching at Questrom",
          "Python makes data analytics fun!"]



#######################################  Named Entity Recognition
## 
## We have seen regex can be very powerful
## not only can we tokenize data, but we COULD use it to parse patterns
##
## HOWEVER:  the spacy parsing has already trained a GENERALIZED model for us
##           lets start there! But note, based on certain tasks, spacy is near/at SOTA
## 
## https://spacy.io/usage/linguistic-features#named-entities
## 
## Why does this matter?
## - we have large corpora and want to extract the entities being discussed
## - think legal documents -  which people/organizations are involved
## - news organizations tagging/categorizing articles to compare across all articles
## - content recommendations - other texts including this entity/entities
## - customer support - which products/services are our customers reference in service requests
## - medical - illnesses or diseases per medical intake forms
## - hiring/scanning: skill detection, experience detection
##

# lets use a different corpus
ner_corpus = ["Apple makes the iphone", 
              "Google created Colab", 
              "Questrom is a B-school",
              "Salesforce acquired Slack for $27 Billion dollars",
              "Mark Benioff leads Salesforce which is located in San Francisco",
              "Admithub just raised $14 million and is located in Boston"]

ner_corpus

# using enumerate 

# ents = []
# for i, d in enumerate(ner_corpus):
#   doc = nlp(d)
#   for ent in doc.ents:
#     ents.append((i, ent.start_char, ent.end_char, ent.text, ent.label_))

# ents

# of course, we can visualize this.  spacy is the bees knees

# displacy.render(nlp(ner_corpus[-1]), style="ent", jupyter=True)

# or in vs code -- localhost:5000
# appears to be a bug with the admithub parse, so beware
# displacy.serve(nlp(ner_corpus[-1]), style="ent")

# so why does this matter?
# lets create a quick corpus about go

# goents = []
# for d in corpus:
#   doc = nlp(d)
#   for ent in doc.ents:
#     goents.append((ent.text, ent.label_)) 

# goents

#######################################  YOUR TURN
##
## parse the article at the URL below
## trick: consider this a document, not a corpus
## extract the entities
## visualize

URL = "https://www.lyrics.com/lyric/180684/Billy+Joel/We+Didn%27t+Start+the+Fire"



######## where to go from here?
##
## spacy attempts to provide us a framework for many NLP tasks
## we chose the medium model to see that the starting point is pretty good
## but its not perfect (it's a model-based approach, after all!)
## 
## the docs are great, and we can role our own, because this is a framework
##

#######################################  Vectors/Embeddings
##
## You have heard me use this term quite a bit
## we have seen this via PCA ----> take a large feature space and re-represent this in a new space
##     the goal was to encode information and reduce noise, right?
##
## we saw this in Tsne (2 embeddings) and UMAP (can be 2 or more depending on our needs)
## 
## Well in text, we have the same idea
## we could always use the tools above, but there this is a "hot" field right now -> embeddings
## 
## https://spacy.io/usage/linguistic-features#vectors-similarity
##
## we will build our own domain-specific embeddings next week, but for now lets use pre-trained embeddings
## let's loosely refer to this as "transfer learning"   --> we are taking one learned model and applying it to our own problem
## in truth, these are generalized, but we are starting to see patterns where domain-specific actions MIGHT help
##
## going back to the start - we used the medium model from spacy to get access to a larger
## trained vocabulary and these embeddings!
##
## I am sure you are thinking: what was this trained on by now:
## https://spacy.io/models/en#en_core_web_md
## view the source (conversations, news articles, texts, etc.)
##

"""![](https://miro.medium.com/max/2224/0*K5a1Ws_nsbEjhbYk.png)

> Above we can see words can be represented in these highly dimensional spaces.  The aim is to encapsulate context.  Remember bag-of-words removes sequence/order!

---
![](https://jalammar.github.io/images/word2vec/king-analogy-viz.png)
"""

# lets see this at the core



# each token has a vector representation

# lets see this for a document

# get the token and the vectors

# lets look at the last entry - Boston

# how many entries in the word vector

# lets look at the entries
# norm = the square root of the sum of the values squared

# explore = []
# doc = nlp(msg)
# for i, token in enumerate(doc):
#   explore.append((i, token.text, token.is_oov, token.has_vector, token.vector_norm))



# spacy has this really nice property, but differs from other approaches!
# Not all tokens have vectors (to save space), but also, when a vector is not available (or because OOV)
# spacy gives us a 300-length vector anyway
# if the token does not have a vector, it will initialize with all 0's.  
# I tend to like this approach, but its not the same for other toolkits where an OOV is just missing

## lets see another example - a little drawn out, but aim is to build intuition

# msg = "Chess is a game, python is a programming language"
# doc = nlp(msg)
# tokens = [token.text for token in doc]
# vectors = [token.vector for token in doc]



# vectors looks awfuly compatible with numpy, dont they.

# va = np.array(vectors)

# from scipy.spatial.distance import pdist, squareform

# cd = pdist(va, metric="cosine")

# squareform(cd).shape

# squareform(cd)[:5, :5]

# or two tokens -- long winded way to get the vectors, 
# we will see an easier way below

# chess = va[0, :]
# python = va[5,:]

# stack the vectors row-wise (now 2 "rows" by 300 "features/columns")
# cp = np.vstack((chess,python))

# calculate sim, not the default distance metric!
# 1 - pdist(cp, metric="cosine")

# lets confirm the intuition with spacy



# spacy compares similarity via cosine

# spacy has a built in cosine SIM (not difference) calc built-in for tokens/docs/spans
# 
# above chess2 and python2 are a doc of a single token
# docs/span vectors are simply the average of the token vectors!
# yes, its that simple
#

# lets compare 3 docs by changing tokens


# print(f"doc 1 and doc 2 is {doc1.similarity(doc2)}") 
# print(f"doc 1 and doc 3 is {doc1.similarity(doc3)}") 
# print(f"doc 2 and doc 3 is {doc2.similarity(doc3)}")

# spans ---> just like slicing a list
#

# the span, at the lowest level, is still comprised of tokens
# and has a vector (average of the span tokens)

#########################################
######################################### Lets see this in action
######################################### USE-CASE 1
#####
#### word vectors and document categories
####

## a pipeline is only the bits that we need (just vectors, for example)
## for a list
# https://spacy.io/usage/processing-pipelines#built-in

# we are only to include the vectors
# nlp = spacy.load("en_core_web_md", enable=['toc2vec'])

## get the topics data from big query
## questrom.datasets.topics
##
## 
# SQL = "SELECT * FROM `questrom.datasets.topics`"

# what do we have

# what is the distro of the intents?

# above we can view thats as relatively evenly distributed customer intents
# for example, frame this as an email coming into support@....

# remember, we setup the tok2vec which only grabs the vectors, not the other components
# https://spacy.io/usage/processing-pipelines#built-in
#

# takes a few minutes
# docs = list(nlp.pipe(intents.text))
# vectors = [doc.vector for doc in docs]
# vectors = np.array(vectors)

# what do we have

# lets throw a PCA at this to start, only care
# about two dimensions for viz
# GOAL? -> can we sort this even with something like PCA

# from sklearn.decomposition import PCA
# pca = PCA(2)
# pcs = pca.fit_transform(vectors)

# pcs.shape

# dataframe
# pcdf = pd.DataFrame(pcs, columns=['pc1','pc2'])
# pcdf['intent'] = intents.topic

pcdf.head(3)

# lets plot this out
# plt.figure(figsize=(10,6))
# p = sns.scatterplot(x="pc1", y="pc2", data=pcdf, hue="intent", alpha=.25)
# p.legend(loc='center left', bbox_to_anchor=(1, 0.5))

# lets take a given statement

# now lets build a reco engine!
# lets take the second row, and compare



# lets grab the second row, or the index 1

# example = squareform(vcs)[1, :]

# now, lets find the top 5 indices
# minimize distance here, not similarity
# so the sort and top 5 let the records with the lowest "distance" values

# sims = np.argsort(example)[:5]

# we can flag similar intents (of course, itself is found)
# this is a function of how I am doing it, but intuition holds we can 
# use this to look up plan of action given similar intents
# this could be a news article, etc.

# intents.iloc[sims, :].values

############################### Challenge/Practice
## a dataset for intents - think re-reouting/optimizing customer service requests!
## small utterances for airlines
## 
## 
## questrom.datasets.airline-intents
## just shy of 5k intents for airline travel/support
##
## lets frame a business problem
##

## each message takes 5 minutes on average to resolve.  
## it costs $300 on average, to resolve an hour's worth of support requests
## this is a manual process today
## can we predict the intent which allows us to pass a first-pass reponse message
## this might resolve the issue 10-20% of the time if we are lucky, but that is a significant time savings

## things to explore
## what are the top 5 locations mentioend
## can you predict the entity?

