{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10 - spacy - NER - Vectors - Classification - Evening",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupR1hF3N9Gm"
      },
      "source": [
        "##############################################################################\n",
        "## Fundamentals for pratical Text Analytics - spacy for language modeling, NER, \n",
        "##                                          roll our own intent sclassification \n",
        "##\n",
        "## Learning goals:\n",
        "##                 - reinforce text as a robust dataset via language modeling\n",
        "##                 - python packages for handling our corpus for these specific tasks\n",
        "##                 - SPACY!\n",
        "##                 - POS tagging (to help with extraction/classification)\n",
        "##                 - NER extraction\n",
        "##                 - generalized, pre-trained word vectors for S|UML tasks (intent classification)\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ge-p9evSd75"
      },
      "source": [
        "# installs\n",
        "! pip install newspaper3k\n",
        "! pip install spacy\n",
        "! pip install wordcloud\n",
        "! pip install emoji\n",
        "! pip install nltk\n",
        "! pip install scikit-plot\n",
        "! pip install umap-learn\n",
        "! pip install afinn\n",
        "! pip install textblob\n",
        "! pip install gensim\n",
        "! pip install pysrt\n",
        "! pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXSNn65zS4el"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplot\n",
        "\n",
        "# some \"fun\" packages\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "\n",
        "import re\n",
        "\n",
        "# text imports\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import gensim\n",
        "\n",
        "from afinn import Afinn\n",
        "\n",
        "from newspaper import Article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHTQKc6dYP7N"
      },
      "source": [
        "#######################################  Quick warmup exercise\n",
        "\n",
        "corpus = ['My email address is btibert@bu.edu',\n",
        "          'I search https://www.google.com 100 times a day!',\n",
        "          'The cat in the hat']\n",
        "\n",
        "\n",
        "# parse the corpus and get the token text, check if it each is a stopword, a url, or an email address\n",
        "# HINT:  refer to the token attribute documentation or inspect a test token\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_jU8LtnuV7H"
      },
      "source": [
        "# import spacy.cli \n",
        "# spacy.cli.download(\"en_core_web_md\")\n",
        "# nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7df4OlXKuV4n"
      },
      "source": [
        "# https://spacy.io/api/token#attributes\n",
        "\n",
        "token_data = []\n",
        "for d in corpus:\n",
        "  doc = nlp(d)\n",
        "  for token in doc:\n",
        "    token_data.append((token.text, token.is_stop, token.like_url, token.like_email))\n",
        "\n",
        "\n",
        "token_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpeknC7buV1W"
      },
      "source": [
        "# what do we have avilable\n",
        "# doc = nlp(\"hockey\")\n",
        "# token = doc[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-3s2ktqMuVwr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxt7C8zDp_Oc"
      },
      "source": [
        "#######################################  Named Entity Recognition\n",
        "## \n",
        "## We have seen regex can be very powerful\n",
        "## not only can we tokenize data, but we COULD use it to parse patterns\n",
        "##\n",
        "## HOWEVER:  the spacy parsing has already trained a GENERALIZED model for us\n",
        "##           lets start there! But note, based on certain tasks, spacy is near/at SOTA\n",
        "## \n",
        "## https://spacy.io/usage/linguistic-features#named-entities\n",
        "## \n",
        "## Why does this matter?\n",
        "## - we have large corpora and want to extract the entities being discussed\n",
        "## - think legal documents -  which people/organizations are involved\n",
        "## - news organizations tagging/categorizing articles to compare across all articles\n",
        "## - content recommendations - other texts including this entity/entities\n",
        "## - customer support - which products/services are our customers reference in service requests\n",
        "## - medical - illnesses or diseases per medical intake forms\n",
        "## - hiring/scanning: skill detection, experience detection\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZJYaIf8AyAJ"
      },
      "source": [
        "# lets use a different corpus\n",
        "ner_corpus = [\"Apple makes the iphone\", \n",
        "              \"Google created Colab\", \n",
        "              \"Questrom is a B-school\",\n",
        "              \"Salesforce acquired Slack for $27 Billion dollars\",\n",
        "              \"Mark Benioff leads Salesforce which is located in San Francisco\",\n",
        "              \"Admithub just raised $14 million and is located in Boston\"]\n",
        "\n",
        "ner_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdU1WxUCuiT9"
      },
      "source": [
        "# using enumerate \n",
        "\n",
        "ents = []\n",
        "for i, d in enumerate(ner_corpus):\n",
        "  doc = nlp(d)\n",
        "  for ent in doc.ents:\n",
        "    ents.append((i, ent.start_char, ent.end_char, ent.text, ent.label_))\n",
        "\n",
        "ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4o4pC4NzQoe"
      },
      "source": [
        "spacy.explain('ORG')\n",
        "spacy.explain('GPE')\n",
        "spacy.explain('PERSON')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaxf9PxAzlhw"
      },
      "source": [
        "# make it dataframe\n",
        "ents_df = pd.DataFrame(ents, columns=['index', 'start', 'end', 'textspan', 'type'])\n",
        "ents_df.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCcc7mrruiWg"
      },
      "source": [
        "# of course, we can visualize this.  spacy is the bees knees\n",
        "\n",
        "from spacy import displacy\n",
        "\n",
        "displacy.render(nlp(ner_corpus[-1]), style=\"ent\", jupyter=True)\n",
        "\n",
        "# or in vs code -- localhost:5000\n",
        "# appears to be a bug with the admithub parse, so beware\n",
        "# displacy.serve(nlp(ner_corpus[-1]), style=\"ent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi481MNQhDLJ"
      },
      "source": [
        "# so why does this matter?\n",
        "# lets create a quick corpus about go\n",
        "\n",
        "corpus = ['I want to go to the store', 'I like programming in the language go']\n",
        "\n",
        "goents = []\n",
        "for d in corpus:\n",
        "  doc = nlp(d)\n",
        "  for ent in doc.ents:\n",
        "    goents.append((ent.text, ent.label_)) \n",
        "\n",
        "goents\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fibYKJOvuiYj"
      },
      "source": [
        "#######################################  YOUR TURN\n",
        "##\n",
        "## parse the article at the URL below\n",
        "## trick: consider this a document, not a corpus\n",
        "## extract the entities\n",
        "## visualize\n",
        "\n",
        "URL = \"https://www.lyrics.com/lyric/180684/Billy+Joel/We+Didn%27t+Start+the+Fire\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFnYId4A1dN8"
      },
      "source": [
        "# parse the article\n",
        "article = Article(URL)\n",
        "article.download()\n",
        "article.parse()\n",
        "article.text\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjS6_Cap1dRv"
      },
      "source": [
        "# extract\n",
        "ents_song = []\n",
        "doc = nlp(article.text)\n",
        "for e in doc.ents:\n",
        "  ents_song.append((e.text, e.start_char, e.end_char, e.label_))\n",
        "\n",
        "len(ents_song)\n",
        "\n",
        "ents_song[:10]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_eURsfMa1dV3"
      },
      "source": [
        "# put this into a dataframe\n",
        "song_enttiies = pd.DataFrame(ents_song, columns=(\"text\", \"start\", \"end\", \"label\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jLkcnOE1dco"
      },
      "source": [
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFYNrNaay0Vt"
      },
      "source": [
        "######## where to go from here?\n",
        "##\n",
        "## spacy attempts to provide us a framework for many NLP tasks\n",
        "## we chose the medium model to see that the starting point is pretty good\n",
        "## but its not perfect (it's a model-based approach, after all!)\n",
        "## \n",
        "## the docs are great, and we can role our own, because this is a framework\n",
        "##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2XwrEWLrJD0"
      },
      "source": [
        "#######################################  Vectors/Embeddings\n",
        "##\n",
        "## You have heard me use this term quite a bit\n",
        "## we have seen this via PCA ----> take a large feature space and re-represent this in a new space\n",
        "##     the goal was to encode information and reduce noise, right?\n",
        "##\n",
        "## we saw this in Tsne (2 embeddings) and UMAP (can be 2 or more depending on our needs)\n",
        "## \n",
        "## Well in text, we have the same idea\n",
        "## we could always use the tools above, but there this is a \"hot\" field right now -> embeddings\n",
        "## \n",
        "## https://spacy.io/usage/linguistic-features#vectors-similarity\n",
        "##\n",
        "## we will build our own domain-specific embeddings next week, but for now lets use pre-trained embeddings\n",
        "## let's loosely refer to this as \"transfer learning\"   --> we are taking one learned model and applying it to our own problem\n",
        "## in truth, these are generalized, but we are starting to see patterns where domain-specific actions MIGHT help\n",
        "##\n",
        "## going back to the start - we used the medium model from spacy to get access to a larger\n",
        "## trained vocabulary and these embeddings!\n",
        "##\n",
        "## I am sure you are thinking: what was this trained on by now:\n",
        "## https://spacy.io/models/en#en_core_web_md\n",
        "## view the source (conversations, news articles, texts, etc.)\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1xYSxqJrJHk"
      },
      "source": [
        "![](https://miro.medium.com/max/2224/0*K5a1Ws_nsbEjhbYk.png)\n",
        "\n",
        "> Above we can see words can be represented in these highly dimensional spaces.  The aim is to encapsulate context.  Remember bag-of-words removes sequence/order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuPrX_1YrJKL"
      },
      "source": [
        "---\n",
        "![](https://jalammar.github.io/images/word2vec/king-analogy-viz.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8a39xpklD4n"
      },
      "source": [
        "# lets see this at the core\n",
        "nlp(\"golf\").vector.shape\n",
        "nlp(\"analytics\").vector[:5]\n",
        "\n",
        "\n",
        "# each token has a vector representation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGBSE3C8rJMU"
      },
      "source": [
        "# lets see this for a document\n",
        "msg = \"Questrom is a business school located in Boston\"\n",
        "vectors = [(doc.text, doc.vector) for doc in nlp(msg)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEKgqJMC8fju"
      },
      "source": [
        "# get the token and the vectors\n",
        "vectors[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibYrMTo4Rcd"
      },
      "source": [
        "# lets look at the last entry - Boston\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgncAPUs4RfC"
      },
      "source": [
        "# how many entries in the word vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmO92nq34RhX"
      },
      "source": [
        "# lets look at the entries\n",
        "# norm = the square root of the sum of the values squared\n",
        "\n",
        "explore = []\n",
        "doc = nlp(msg)\n",
        "for i, token in enumerate(doc):\n",
        "  explore.append((i, token.text, token.is_oov, token.has_vector, token.vector_norm))\n",
        "\n",
        "explore"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLgNg7v6jFG"
      },
      "source": [
        "nlp(\"Questrom\").vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKPQvRK6jIr"
      },
      "source": [
        "# spacy has this really nice property, but differs from other approaches!\n",
        "# Not all tokens have vectors (to save space), but also, when a vector is not available (or because OOV)\n",
        "# spacy gives us a 300-length vector anyway\n",
        "# if the token does not have a vector, it will initialize with all 0's.  \n",
        "# I tend to like this approach, but its not the same for other toolkits where an OOV is just missing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTRKXvqH6jLJ"
      },
      "source": [
        "## lets see another example - a little drawn out, but aim is to build intuition\n",
        "\n",
        "msg = \"Chess is a game, python is a programming language\"\n",
        "doc = nlp(msg)\n",
        "tokens = [token.text for token in doc]\n",
        "vectors = [token.vector for token in doc]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaBwEMQvodUA"
      },
      "source": [
        "vectors[-0][:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEWYBa51ZRr1"
      },
      "source": [
        "# vectors looks awfuly compatible with numpy, dont they.\n",
        "\n",
        "va = np.array(vectors)\n",
        "\n",
        "from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "cd = pdist(va, metric=\"cosine\")\n",
        "\n",
        "squareform(cd).shape\n",
        "\n",
        "squareform(cd)[:5, :5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adZe03jo4hh"
      },
      "source": [
        "# or two tokens -- long winded way to get the vectors, \n",
        "# we will see an easier way below\n",
        "\n",
        "chess = va[0, :]\n",
        "python = va[5,:]\n",
        "\n",
        "# stack the vectors row-wise (now 2 \"rows\" by 300 \"features/columns\")\n",
        "cp = np.vstack((chess,python))\n",
        "cp.shape\n",
        "\n",
        "# calculate sim, not the default distance metric!\n",
        "1 - pdist(cp, metric=\"cosine\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNYIL1r1pN-u"
      },
      "source": [
        "# lets confirm the intuition with spacy\n",
        "\n",
        "chess2 = nlp(\"chess\")\n",
        "python2 = nlp(\"python\")\n",
        "\n",
        "\n",
        "\n",
        "# spacy compares similarity via cosine\n",
        "chess2.similarity(python2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwD92VchZR0R"
      },
      "source": [
        "# spacy has a built in cosine SIM (not difference) calc built-in for tokens/docs/spans\n",
        "# \n",
        "# above chess2 and python2 are a doc of a single token\n",
        "# docs/span vectors are simply the average of the token vectors!\n",
        "# yes, its that simple\n",
        "#\n",
        "\n",
        "# lets compare 3 docs by changing tokens\n",
        "doc1 = nlp(\"I like turtles\")\n",
        "doc2 = nlp(\"I like hockey\")\n",
        "doc3 = nlp(\"I hate hockey\")\n",
        "\n",
        "print(f\"doc 1 and doc 2 is {doc1.similarity(doc2)}\") \n",
        "print(f\"doc 1 and doc 3 is {doc1.similarity(doc3)}\") \n",
        "print(f\"doc 2 and doc 3 is {doc2.similarity(doc3)}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRG1Kgz2ZR2u"
      },
      "source": [
        "# spans ---> just like slicing a list\n",
        "# \n",
        "\n",
        "doc1[:2].vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA0ZZRveZR6D"
      },
      "source": [
        "# the span, at the lowest level, is still comprised of tokens\n",
        "# and has a vector (average of the span tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOyHsvkI9ADy"
      },
      "source": [
        "#########################################\n",
        "######################################### Lets see this in action\n",
        "######################################### USE-CASE 1\n",
        "#####\n",
        "#### word vectors and document categories\n",
        "####\n",
        "\n",
        "## a pipeline is only the bits that we need (just vectors, for example)\n",
        "## for a list\n",
        "# https://spacy.io/usage/processing-pipelines#built-in\n",
        "\n",
        "# we are only to include the vectors\n",
        "nlp = spacy.load(\"en_core_web_md\", enable=['toc2vec'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eba_Vlh4Kvm3"
      },
      "source": [
        "## get the topics data from big query\n",
        "## questrom.datasets.topics\n",
        "##\n",
        "## \n",
        "\n",
        "SQL = \"SELECT * FROM `questrom.datasets.topics`\"\n",
        "PROJ = \"questrom\"\n",
        "\n",
        "intents = pd.read_gbq(SQL, PROJ)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJI2RxRULSQK"
      },
      "source": [
        "# what do we have\n",
        "intents.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rC8Fwb99TjN"
      },
      "source": [
        "intents.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEC_4Ac5LSMM"
      },
      "source": [
        "# what is the distro of the intents?\n",
        "intents.topic.value_counts(normalize=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw21luVgLSIh"
      },
      "source": [
        "# above we can view thats as relatively evenly distributed customer intents\n",
        "# for example, frame this as an email coming into support@....\n",
        "\n",
        "docs = list(nlp.pipe(intents.text))\n",
        "vectors = [doc.vector for doc in docs]\n",
        "vectors = np.array(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atMADPfRMX61"
      },
      "source": [
        "# remember, we setup the tok2vec which only grabs the vectors, not the other components\n",
        "# https://spacy.io/usage/processing-pipelines#built-in\n",
        "#\n",
        "\n",
        "# takes a few minutes\n",
        "# docs = list(nlp.pipe(intents.text))\n",
        "# vectors = [doc.vector for doc in docs]\n",
        "# vectors = np.array(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbF7L1yeMX-s"
      },
      "source": [
        "# what do we have\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EunmzWioMYCf"
      },
      "source": [
        "# lets throw a PCA at this to start, only care\n",
        "# about two dimensions for viz\n",
        "# GOAL? -> can we sort this even with something like PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(2)\n",
        "pcs = pca.fit_transform(vectors)\n",
        "\n",
        "pcs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJnTxDNMYE9"
      },
      "source": [
        "# dataframe\n",
        "pcdf = pd.DataFrame(pcs, columns=['pc1','pc2'])\n",
        "pcdf['intent'] = intents.topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdWFFXMvMYHc"
      },
      "source": [
        "pcdf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1daL4EILSCP"
      },
      "source": [
        "# lets plot this out\n",
        "plt.figure(figsize=(10,6))\n",
        "p = sns.scatterplot(x=\"pc1\", y=\"pc2\", data=pcdf, hue=\"intent\", alpha=.25)\n",
        "p.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPFKwn1epOQd"
      },
      "source": [
        "# lets take a given statement\n",
        "intents.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wyak7D-pb5Q"
      },
      "source": [
        "# now lets build a reco engine!\n",
        "# lets take the second row, and compare\n",
        "\n",
        "vcs = pdist(vectors, metric=\"cosine\")\n",
        "\n",
        "# lets grab the second row, or the index 1\n",
        "\n",
        "example = squareform(vcs)[1, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKeeWKlpb2M"
      },
      "source": [
        "# now, lets find the top 5 indices\n",
        "# minimize distance here, not similarity\n",
        "# so the sort and top 5 let the records with the lowest \"distance\" values\n",
        "\n",
        "sims = np.argsort(example)[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ6WnQnOpbyP"
      },
      "source": [
        "# we can flag similar intents (of course, itself is found)\n",
        "# this is a function of how I am doing it, but intuition holds we can \n",
        "# use this to look up plan of action given similar intents\n",
        "# this could be a news article, etc.\n",
        "\n",
        "intents.iloc[sims, :].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsfoBPcS4RlN"
      },
      "source": [
        "############################### Challenge/Practice\n",
        "## a dataset for intents - think re-reouting/optimizing customer service requests!\n",
        "## small utterances for airlines\n",
        "## \n",
        "## \n",
        "## questrom.datasets.airline-intents\n",
        "## just shy of 5k intents for airline travel/support\n",
        "##\n",
        "## lets frame a business problem\n",
        "##\n",
        "\n",
        "## each message takes 5 minutes on average to resolve.  \n",
        "## it costs $300 on average, to resolve an hour's worth of support requests\n",
        "## this is a manual process today\n",
        "## can we predict the intent which allows us to pass a first-pass reponse message\n",
        "## this might resolve the issue 10-20% of the time if we are lucky, but that is a significant time savings\n",
        "\n",
        "## things to explore\n",
        "## what are the top 5 locations mentioend\n",
        "## can you predict the entity?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3i7oneN8fxm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}