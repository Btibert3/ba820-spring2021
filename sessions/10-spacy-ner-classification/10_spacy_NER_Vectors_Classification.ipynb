{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "10 - spacy - NER - Vectors - Classification",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hupR1hF3N9Gm"
      },
      "source": [
        "##############################################################################\n",
        "## Fundamentals for pratical Text Analytics - spacy for language modeling, NER, \n",
        "##                                          roll our own intent sclassification \n",
        "##\n",
        "## Learning goals:\n",
        "##                 - reinforce text as a robust dataset via language modeling\n",
        "##                 - python packages for handling our corpus for these specific tasks\n",
        "##                 - SPACY!\n",
        "##                 - POS tagging (to help with extraction/classification)\n",
        "##                 - NER extraction\n",
        "##                 - generalized, pre-trained word vectors for S|UML tasks (intent classification)\n",
        "##############################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ge-p9evSd75"
      },
      "source": [
        "# installs\n",
        "! pip install newspaper3k\n",
        "! pip install spacy\n",
        "! pip install wordcloud\n",
        "! pip install emoji\n",
        "! pip install nltk\n",
        "! pip install scikit-plot\n",
        "! pip install umap-learn\n",
        "! pip install afinn\n",
        "! pip install textblob\n",
        "! pip install gensim\n",
        "! pip install pysrt\n",
        "! pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXSNn65zS4el"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import scikitplot as skplot\n",
        "\n",
        "# some \"fun\" packages\n",
        "from wordcloud import WordCloud\n",
        "import emoji\n",
        "\n",
        "import re\n",
        "\n",
        "# text imports\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer,TfidfVectorizer  \n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import gensim\n",
        "\n",
        "from afinn import Afinn\n",
        "\n",
        "from newspaper import Article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WiTrz7hhafGg"
      },
      "source": [
        "##################################### WARMUP Exercise\n",
        "##################################### ~ 10\n",
        "## there is a file located below: \n",
        "##     https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt\n",
        "## there is also some started code below\n",
        "## calculate the sentiment over the course of the movie script (Shrek)\n",
        "## plot the sentiment arc over the movie\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXH25lavcpS1"
      },
      "source": [
        "# 0. get the file - the file in the browser will auto download\n",
        "#    just make sure you have the file in your working directory\n",
        "\n",
        "# or if on colab\n",
        "! wget https://storage.googleapis.com/qst-datasets/subtitles/Shrek-2001.srt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-d1CJKQafKe"
      },
      "source": [
        "# 1. get the file and parse\n",
        "import pysrt\n",
        "subs = pysrt.open('Shrek-2001.srt', encoding='iso-8859-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ciZj-udvYaq7"
      },
      "source": [
        "##################################### Quick example \n",
        "##################################### Key Words in Context - Concordance\n",
        "###\n",
        "### powerful tool to look at a set of text (full corpus) and look for\n",
        "### words before/after\n",
        "###\n",
        "### helpful for eda, look for patterns to help support data annotation, etc.\n",
        "\n",
        "\n",
        "# just in case\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# get the data\n",
        "# SQL = \"SELECT * FROM `questrom.datasets.topics`\"\n",
        "# PROJ = \"questrom\"\n",
        "# intents = pd.read_gbq(SQL, PROJ)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCX9GtBudZaZ"
      },
      "source": [
        "# text\n",
        "# from nltk import Text\n",
        "\n",
        "# get the text into the correct format\n",
        "# intents_text = intents.text.tolist()\n",
        "# print(len(intents_text))\n",
        "\n",
        "# put into a corpus (as if it were 1 big file, ignoring that we have intents)\n",
        "# corpus = \" \".join(intents_text)\n",
        "\n",
        "# tokenize\n",
        "# tokens = nltk.word_tokenize(corpus)\n",
        "# len(tokens)\n",
        "\n",
        "# put the corpus into a Text object.  \n",
        "# some nice features when we consider the corpus a blob of text that lacks \n",
        "# structure like sentences, or by user, etc.  Just the text combined like a book chapter.\n",
        "# text = Text(tokens)\n",
        "\n",
        "# look for the context of words\n",
        "# Key work in context, or concordance\n",
        "\n",
        "## shipping / help  / size / discount\n",
        "# text.concordance(\"product\", width=80, lines=10)\n",
        "\n",
        "# above is referred to as key words in context via quanteda in R"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knvwOULfdNFT"
      },
      "source": [
        "##################################### REFERENCE AND THOUGHT MATERIAL: Emojis\n",
        "## \n",
        "##\n",
        "## get the emoji sentiment database here: http://kt.ijs.si/data/Emoji_sentiment_ranking/ \n",
        "\n",
        "## HINT:  pandas can parse web tables, remember! via read_html, and then we have a list of dataframes\n",
        "## \n",
        "## \n",
        "## sources of the datasets (check out the about)\n",
        "## real source is: https://www.clarin.si/repository/xmlui/handle/11356/1048\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvDEFXC6dM-f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5nLzTyTYati"
      },
      "source": [
        "######################################### lets get started with spacy!\n",
        "##\n",
        "## \"industrial strength\" NLP tools\n",
        "## increasingly the go to in this space, but like Textblob, \n",
        "## it focuses on document-oriented operations\n",
        "## lots of configuration and power, but lets get the foundations set\n",
        "##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tce4jGgBdMUV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQIByvjyahDi"
      },
      "source": [
        "# setup spacy - I am using CLI below because I think its easier and works on colab too\n",
        "# you can see this done via something like below\n",
        "# \n",
        "# from spacy.lang.en import English\n",
        "# nlp = English()\n",
        "# \n",
        "\n",
        "# If VS Code crashes\n",
        "# use\n",
        "# (from command line python -m spacy download en_core_web_md)\n",
        "# and ignore the cli commands below, proceed to spacy.load\n",
        "\n",
        "# import spacy.cli \n",
        "\n",
        "# https://spacy.io/models/en\n",
        "# download the medium model - not the sm model\n",
        "# spacy.cli.download(\"en_core_web_md\")\n",
        "\n",
        "\n",
        "# spacy operates via language models\n",
        "# https://spacy.io/models/en\n",
        "# we are going to setup the model as nlp, the spacy convention\n",
        "\n",
        "# this will take a few moments\n",
        "# most examples start with small, but we are going to use a larger model\n",
        "# nlp = spacy.load('en_core_web_md')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2iCB7tnYaws"
      },
      "source": [
        "# the message we will parse into a document\n",
        "\n",
        "\n",
        "# we are using all of the defaults, but this is a \"heavy\" parse with a ton of great data to explore\n",
        "# https://spacy.io/usage/spacy-101#features\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9x7ebFfpYazb"
      },
      "source": [
        "# what do we have?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVrSEAx1byU-"
      },
      "source": [
        "# notice that we have tokens and from there a document\n",
        "#\n",
        "# as we have, check out what we can do\n",
        "# doc tab, or shift tab\n",
        "\n",
        "# doc.\n",
        "# or you can use dir(doc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HJdwilJdUsM"
      },
      "source": [
        "# length\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhbe3NoidYNb"
      },
      "source": [
        "# we can list it of course\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rU6-cAdixR"
      },
      "source": [
        "# but this is so much more than a simple string!\n",
        "# \n",
        "# the tokens have properties/attributes\n",
        "# https://spacy.io/usage/linguistic-features#tokenization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6yAvgcfdmcR"
      },
      "source": [
        "# also, when tokenized, all sorts of atributes\n",
        "# quick example to get rid of the punct as just one example of inspection\n",
        "\n",
        "# tokens = [token.text for token in doc if not token.is_punct]\n",
        "# tokens\n",
        "\n",
        "## of course there is so much you can do here\n",
        "## \n",
        "## but lets just focus on spacy suggesting what they believe are best practices by default (keep punct)\n",
        "## \n",
        "## just like we have seen, we can use the language model to parse tokens, and \n",
        "## pass that to sklearn, but that is wildly under-utilizing spacy\n",
        "##\n",
        "## But spacy has all sorts of great features, lets explore!\n",
        "##\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOSUXsFAySf"
      },
      "source": [
        "# lets see what this NLP model is all above\n",
        "# parsed = []\n",
        "# for token in doc:\n",
        "#   parsed.append((token.text, token.lemma_, token.pos_, token.tag_, token.dep_))\n",
        "\n",
        "# parsed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cnh1AmNAyQY"
      },
      "source": [
        "# ok, I am the farthest thing from a linguist, \n",
        "# https://spacy.io/usage/linguistic-features#pos-tagging\n",
        "#\n",
        "# during lemma, this might help: \n",
        "# https://stackoverflow.com/questions/50543752/spacy-lemmatization-on-pronouns-gives-some-erronous-output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MAY2dfm_AyMO"
      },
      "source": [
        "# luckily we can use explain\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaGxMjk6AyJr"
      },
      "source": [
        "# spacy, via it's models (learned from labeled data!) can even map the relationships\n",
        "# from spacy import displacy\n",
        "\n",
        "# for those in jupyter or colab\n",
        "# displacy.render(doc, style=\"dep\", jupyter=True)\n",
        "\n",
        "# if in vs code - spacy has a built in server\n",
        "# the viz is at http://localhost:5000 \n",
        "# displacy.serve(doc, style=\"dep\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A44HwVTYAyGp"
      },
      "source": [
        "#######################################  YOUR TURN\n",
        "# using the corpus below, parse the text and POS tag, and lemma\n",
        "\n",
        "corpus = [\"Brock likes hockey\",\n",
        "          \"I love teaching at Questrom\",\n",
        "          \"Python makes data analytics fun!\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulMLxr0K_STH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxt7C8zDp_Oc"
      },
      "source": [
        "#######################################  Named Entity Recognition\n",
        "## \n",
        "## We have seen regex can be very powerful\n",
        "## not only can we tokenize data, but we COULD use it to parse patterns\n",
        "##\n",
        "## HOWEVER:  the spacy parsing has already trained a GENERALIZED model for us\n",
        "##           lets start there! But note, based on certain tasks, spacy is near/at SOTA\n",
        "## \n",
        "## https://spacy.io/usage/linguistic-features#named-entities\n",
        "## \n",
        "## Why does this matter?\n",
        "## - we have large corpora and want to extract the entities being discussed\n",
        "## - think legal documents -  which people/organizations are involved\n",
        "## - news organizations tagging/categorizing articles to compare across all articles\n",
        "## - content recommendations - other texts including this entity/entities\n",
        "## - customer support - which products/services are our customers reference in service requests\n",
        "## - medical - illnesses or diseases per medical intake forms\n",
        "## - hiring/scanning: skill detection, experience detection\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZJYaIf8AyAJ"
      },
      "source": [
        "# lets use a different corpus\n",
        "ner_corpus = [\"Apple makes the iphone\", \n",
        "              \"Google created Colab\", \n",
        "              \"Questrom is a B-school\",\n",
        "              \"Salesforce acquired Slack for $27 Billion dollars\",\n",
        "              \"Mark Benioff leads Salesforce which is located in San Francisco\",\n",
        "              \"Admithub just raised $14 million and is located in Boston\"]\n",
        "\n",
        "ner_corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdU1WxUCuiT9"
      },
      "source": [
        "# using enumerate \n",
        "\n",
        "# ents = []\n",
        "# for i, d in enumerate(ner_corpus):\n",
        "#   doc = nlp(d)\n",
        "#   for ent in doc.ents:\n",
        "#     ents.append((i, ent.start_char, ent.end_char, ent.text, ent.label_))\n",
        "\n",
        "# ents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCcc7mrruiWg"
      },
      "source": [
        "# of course, we can visualize this.  spacy is the bees knees\n",
        "\n",
        "# displacy.render(nlp(ner_corpus[-1]), style=\"ent\", jupyter=True)\n",
        "\n",
        "# or in vs code -- localhost:5000\n",
        "# appears to be a bug with the admithub parse, so beware\n",
        "# displacy.serve(nlp(ner_corpus[-1]), style=\"ent\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oi481MNQhDLJ"
      },
      "source": [
        "# so why does this matter?\n",
        "# lets create a quick corpus about go\n",
        "\n",
        "# goents = []\n",
        "# for d in corpus:\n",
        "#   doc = nlp(d)\n",
        "#   for ent in doc.ents:\n",
        "#     goents.append((ent.text, ent.label_)) \n",
        "\n",
        "# goents\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fibYKJOvuiYj"
      },
      "source": [
        "#######################################  YOUR TURN\n",
        "##\n",
        "## parse the article at the URL below\n",
        "## trick: consider this a document, not a corpus\n",
        "## extract the entities\n",
        "## visualize\n",
        "\n",
        "URL = \"https://www.lyrics.com/lyric/180684/Billy+Joel/We+Didn%27t+Start+the+Fire\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnadJFlx_hZc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFYNrNaay0Vt"
      },
      "source": [
        "######## where to go from here?\n",
        "##\n",
        "## spacy attempts to provide us a framework for many NLP tasks\n",
        "## we chose the medium model to see that the starting point is pretty good\n",
        "## but its not perfect (it's a model-based approach, after all!)\n",
        "## \n",
        "## the docs are great, and we can role our own, because this is a framework\n",
        "##\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2XwrEWLrJD0"
      },
      "source": [
        "#######################################  Vectors/Embeddings\n",
        "##\n",
        "## You have heard me use this term quite a bit\n",
        "## we have seen this via PCA ----> take a large feature space and re-represent this in a new space\n",
        "##     the goal was to encode information and reduce noise, right?\n",
        "##\n",
        "## we saw this in Tsne (2 embeddings) and UMAP (can be 2 or more depending on our needs)\n",
        "## \n",
        "## Well in text, we have the same idea\n",
        "## we could always use the tools above, but there this is a \"hot\" field right now -> embeddings\n",
        "## \n",
        "## https://spacy.io/usage/linguistic-features#vectors-similarity\n",
        "##\n",
        "## we will build our own domain-specific embeddings next week, but for now lets use pre-trained embeddings\n",
        "## let's loosely refer to this as \"transfer learning\"   --> we are taking one learned model and applying it to our own problem\n",
        "## in truth, these are generalized, but we are starting to see patterns where domain-specific actions MIGHT help\n",
        "##\n",
        "## going back to the start - we used the medium model from spacy to get access to a larger\n",
        "## trained vocabulary and these embeddings!\n",
        "##\n",
        "## I am sure you are thinking: what was this trained on by now:\n",
        "## https://spacy.io/models/en#en_core_web_md\n",
        "## view the source (conversations, news articles, texts, etc.)\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P1xYSxqJrJHk"
      },
      "source": [
        "![](https://miro.medium.com/max/2224/0*K5a1Ws_nsbEjhbYk.png)\n",
        "\n",
        "> Above we can see words can be represented in these highly dimensional spaces.  The aim is to encapsulate context.  Remember bag-of-words removes sequence/order!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuPrX_1YrJKL"
      },
      "source": [
        "---\n",
        "![](https://jalammar.github.io/images/word2vec/king-analogy-viz.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8a39xpklD4n"
      },
      "source": [
        "# lets see this at the core\n",
        "\n",
        "\n",
        "\n",
        "# each token has a vector representation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGBSE3C8rJMU"
      },
      "source": [
        "# lets see this for a document\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEKgqJMC8fju"
      },
      "source": [
        "# get the token and the vectors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CibYrMTo4Rcd"
      },
      "source": [
        "# lets look at the last entry - Boston\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgncAPUs4RfC"
      },
      "source": [
        "# how many entries in the word vector\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmO92nq34RhX"
      },
      "source": [
        "# lets look at the entries\n",
        "# norm = the square root of the sum of the values squared\n",
        "\n",
        "# explore = []\n",
        "# doc = nlp(msg)\n",
        "# for i, token in enumerate(doc):\n",
        "#   explore.append((i, token.text, token.is_oov, token.has_vector, token.vector_norm))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMLgNg7v6jFG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POKPQvRK6jIr"
      },
      "source": [
        "# spacy has this really nice property, but differs from other approaches!\n",
        "# Not all tokens have vectors (to save space), but also, when a vector is not available (or because OOV)\n",
        "# spacy gives us a 300-length vector anyway\n",
        "# if the token does not have a vector, it will initialize with all 0's.  \n",
        "# I tend to like this approach, but its not the same for other toolkits where an OOV is just missing\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTRKXvqH6jLJ"
      },
      "source": [
        "## lets see another example - a little drawn out, but aim is to build intuition\n",
        "\n",
        "# msg = \"Chess is a game, python is a programming language\"\n",
        "# doc = nlp(msg)\n",
        "# tokens = [token.text for token in doc]\n",
        "# vectors = [token.vector for token in doc]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaBwEMQvodUA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEWYBa51ZRr1"
      },
      "source": [
        "# vectors looks awfuly compatible with numpy, dont they.\n",
        "\n",
        "# va = np.array(vectors)\n",
        "\n",
        "# from scipy.spatial.distance import pdist, squareform\n",
        "\n",
        "# cd = pdist(va, metric=\"cosine\")\n",
        "\n",
        "# squareform(cd).shape\n",
        "\n",
        "# squareform(cd)[:5, :5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_adZe03jo4hh"
      },
      "source": [
        "# or two tokens -- long winded way to get the vectors, \n",
        "# we will see an easier way below\n",
        "\n",
        "# chess = va[0, :]\n",
        "# python = va[5,:]\n",
        "\n",
        "# stack the vectors row-wise (now 2 \"rows\" by 300 \"features/columns\")\n",
        "# cp = np.vstack((chess,python))\n",
        "\n",
        "# calculate sim, not the default distance metric!\n",
        "# 1 - pdist(cp, metric=\"cosine\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hNYIL1r1pN-u"
      },
      "source": [
        "# lets confirm the intuition with spacy\n",
        "\n",
        "\n",
        "\n",
        "# spacy compares similarity via cosine\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwD92VchZR0R"
      },
      "source": [
        "# spacy has a built in cosine SIM (not difference) calc built-in for tokens/docs/spans\n",
        "# \n",
        "# above chess2 and python2 are a doc of a single token\n",
        "# docs/span vectors are simply the average of the token vectors!\n",
        "# yes, its that simple\n",
        "#\n",
        "\n",
        "# lets compare 3 docs by changing tokens\n",
        "\n",
        "\n",
        "# print(f\"doc 1 and doc 2 is {doc1.similarity(doc2)}\") \n",
        "# print(f\"doc 1 and doc 3 is {doc1.similarity(doc3)}\") \n",
        "# print(f\"doc 2 and doc 3 is {doc2.similarity(doc3)}\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRG1Kgz2ZR2u"
      },
      "source": [
        "# spans ---> just like slicing a list\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hA0ZZRveZR6D"
      },
      "source": [
        "# the span, at the lowest level, is still comprised of tokens\n",
        "# and has a vector (average of the span tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOyHsvkI9ADy"
      },
      "source": [
        "#########################################\n",
        "######################################### Lets see this in action\n",
        "######################################### USE-CASE 1\n",
        "#####\n",
        "#### word vectors and document categories\n",
        "####\n",
        "\n",
        "## a pipeline is only the bits that we need (just vectors, for example)\n",
        "## for a list\n",
        "# https://spacy.io/usage/processing-pipelines#built-in\n",
        "\n",
        "# we are only to include the vectors\n",
        "# nlp = spacy.load(\"en_core_web_md\", enable=['toc2vec'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eba_Vlh4Kvm3"
      },
      "source": [
        "## get the topics data from big query\n",
        "## questrom.datasets.topics\n",
        "##\n",
        "## \n",
        "# SQL = \"SELECT * FROM `questrom.datasets.topics`\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJI2RxRULSQK"
      },
      "source": [
        "# what do we have\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEC_4Ac5LSMM"
      },
      "source": [
        "# what is the distro of the intents?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw21luVgLSIh"
      },
      "source": [
        "# above we can view thats as relatively evenly distributed customer intents\n",
        "# for example, frame this as an email coming into support@....\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atMADPfRMX61"
      },
      "source": [
        "# remember, we setup the tok2vec which only grabs the vectors, not the other components\n",
        "# https://spacy.io/usage/processing-pipelines#built-in\n",
        "#\n",
        "\n",
        "# takes a few minutes\n",
        "# docs = list(nlp.pipe(intents.text))\n",
        "# vectors = [doc.vector for doc in docs]\n",
        "# vectors = np.array(vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbF7L1yeMX-s"
      },
      "source": [
        "# what do we have\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EunmzWioMYCf"
      },
      "source": [
        "# lets throw a PCA at this to start, only care\n",
        "# about two dimensions for viz\n",
        "# GOAL? -> can we sort this even with something like PCA\n",
        "\n",
        "# from sklearn.decomposition import PCA\n",
        "# pca = PCA(2)\n",
        "# pcs = pca.fit_transform(vectors)\n",
        "\n",
        "# pcs.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mJnTxDNMYE9"
      },
      "source": [
        "# dataframe\n",
        "# pcdf = pd.DataFrame(pcs, columns=['pc1','pc2'])\n",
        "# pcdf['intent'] = intents.topic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdWFFXMvMYHc"
      },
      "source": [
        "pcdf.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1daL4EILSCP"
      },
      "source": [
        "# lets plot this out\n",
        "# plt.figure(figsize=(10,6))\n",
        "# p = sns.scatterplot(x=\"pc1\", y=\"pc2\", data=pcdf, hue=\"intent\", alpha=.25)\n",
        "# p.legend(loc='center left', bbox_to_anchor=(1, 0.5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YPFKwn1epOQd"
      },
      "source": [
        "# lets take a given statement\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wyak7D-pb5Q"
      },
      "source": [
        "# now lets build a reco engine!\n",
        "# lets take the second row, and compare\n",
        "\n",
        "\n",
        "\n",
        "# lets grab the second row, or the index 1\n",
        "\n",
        "# example = squareform(vcs)[1, :]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKeeWKlpb2M"
      },
      "source": [
        "# now, lets find the top 5 indices\n",
        "# minimize distance here, not similarity\n",
        "# so the sort and top 5 let the records with the lowest \"distance\" values\n",
        "\n",
        "# sims = np.argsort(example)[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EZ6WnQnOpbyP"
      },
      "source": [
        "# we can flag similar intents (of course, itself is found)\n",
        "# this is a function of how I am doing it, but intuition holds we can \n",
        "# use this to look up plan of action given similar intents\n",
        "# this could be a news article, etc.\n",
        "\n",
        "# intents.iloc[sims, :].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AsfoBPcS4RlN"
      },
      "source": [
        "############################### Challenge/Practice\n",
        "## a dataset for intents - think re-reouting/optimizing customer service requests!\n",
        "## small utterances for airlines\n",
        "## \n",
        "## \n",
        "## questrom.datasets.airline-intents\n",
        "## just shy of 5k intents for airline travel/support\n",
        "##\n",
        "## lets frame a business problem\n",
        "##\n",
        "\n",
        "## each message takes 5 minutes on average to resolve.  \n",
        "## it costs $300 on average, to resolve an hour's worth of support requests\n",
        "## this is a manual process today\n",
        "## can we predict the intent which allows us to pass a first-pass reponse message\n",
        "## this might resolve the issue 10-20% of the time if we are lucky, but that is a significant time savings\n",
        "\n",
        "## things to explore\n",
        "## what are the top 5 locations mentioend\n",
        "## can you predict the entity?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3i7oneN8fxm"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}