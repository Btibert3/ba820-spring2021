# -*- coding: utf-8 -*-
"""11 - A - DIY Word Embeddings

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EVp3_tTC77VDwswXocc-MkRfDOhdCHo8

# Word Embeddings via Word2Vec

Instead of using pre-trained models, if you really needed to, and had a large corpus, you could train your own embeddings in attempt to provide domain context.

# General Process

- Define our corpus == universe assumption (we have captured the vocabulary well enough in our corpus)
- Spacy to tokenize the text
- List of lists -> gensim:word2vec (a each list has the tokens)
- Gensim Word2Vec
- Tell spacy about our vectors

> We will use a smaller corpus to highlight tradeoffs of training our own
"""

# imports
import numpy as np
import pandas as pd

import spacy
import gensim

# initalize spacy -- we are going to bring our own model
#                    so small is fine (and explicit)

# MODEL = "en_core_web_sm"
# spacy.cli.download(MODEL)

### ok, we have that model, lets build our own
# nlp = spacy.load(MODEL)

# get the data == universe
SQL = "SELECT * from `datasets.airline-intents`"

# get a sample to see what we have

"""## Step 1: spacy tokenize text"""

# tokenize the corpus

# def tokenize(text):
#   doc = nlp(text)
#   return [token.text for token in doc]

# apply it to the text column

# another sample

"""## Step 2 : Gensim Word2Vec"""

# gensim
# from gensim.models import Word2Vec

# extract tokens as a list of lists

# docs = intents.tokens.to_list()

# fit the model 
# 50 feature vectors, a context window of 3, and skipgram model
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec

# model = Word2Vec(docs, size=50, window=3, sg=1)

# what do we have
# type(model)

# we have some basics

# model.corpus_count

# what is the size of our vocab?

# len(model.wv.vocab)

# get vocab vectors

# model.wv.get_vector('the')
# model.wv.get_vector('flight')
# model.wv.get_vector('boston')
# model.wv.get_vector('help')


## it fails hard on a lookup

# we can also compare

# we can look at the most similar vectors for a token

# model.wv.most_similar("boston")
# model.wv.most_similar("flight")
# model.wv.most_similar("the")

"""# Step 3: Save and load into spacy"""

# ok, lets save this out to a text file
# model.wv.save_word2vec_format("word2vec.txt")

# we are going to compres the file

# ! gzip word2vec.txt

# inform spacy of a new model, 
# https://spacy.io/api/cli#init-vectors
# we are on spacy 2

# ! python -m spacy init-model en brock-model --vectors-loc word2vec.txt.gz

# rename this to whatever you have above
# nlp = spacy.load("brock-model")

# lets check the vectors are being used
# check boston

# compare that we are using the same
# model.wv.get_vector("boston")

# compare spacy and gensim are the same
# nlp("boston").vector == model.wv.get_vector("boston")

# all the other bits still apply
# test = nlp("This is the example please btibert@bu.edu")

# lets confirm

# for token in test:
#   print(token.text, token.lemma_, token.like_email, token.is_oov)

## what does above tell us about building our own?
## what might we need to "improve" our model/vectors?

"""## Where to go from here

- fastText vectors [use txt not bin]: https://github.com/facebookresearch/fastText/blob/master/docs/crawl-vectors.md#models

"""

