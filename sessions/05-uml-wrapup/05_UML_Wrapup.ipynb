{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "05 - UML Wrapup",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFMinYePfk-m"
      },
      "source": [
        "######################################################\n",
        "## Wrap up UML\n",
        "## Hands on-heavy class to dive into some concepts you may want to explore \n",
        "## Learning objectives:\n",
        "##\n",
        "## 0. finish up PCA\n",
        "## 1. exposure to more contemporary techniques for interviews, awareness, and further exploration\n",
        "## 2. highlight different use-cases, some that help with viz for non-tech, others to think about alternatives to linear PCA\n",
        "## 3. use this as a jumping off point for you to consider the fact that are lots of methods, and its not typically for this task, do this one approach\n",
        "## 4. hands on application of UML and SML data challenge!\n",
        "##  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp6tyu3Pq6qU"
      },
      "source": [
        "# installs\n",
        "\n",
        "# notebook install\n",
        "# ! pip install umap-learn\n",
        "\n",
        "# local\n",
        "# pip install umap-learn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRgZJtfTjnge"
      },
      "source": [
        "# imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "\n",
        "# scipy\n",
        "from scipy.spatial.distance import pdist\n",
        "\n",
        "# scikit\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn import metrics\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7NzKU5XjhK4"
      },
      "source": [
        "########################################################################\n",
        "# PCA wrapup - quick individual exercise\n",
        "# \n",
        "# Let's get coding!\n",
        "# \n",
        "# 1. get the diamonds dataset from Big Query\n",
        "# 2. keep just the numeric columns, but EXCLUDE price\n",
        "# 3. standardize the columns with Min Max Scaler (tricky!)  <----- for practice purposes\n",
        "# 4. fit a PCA model to the data\n",
        "# 5. plot the variance explained (your choice)\n",
        "# \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erRKGJjv6bXg"
      },
      "source": [
        "#############################  MOVING FORWARD\n",
        "## \n",
        "## some options when fitting PCA Model\n",
        "## we can pre-specify the number of components OR the % of var we want to explain!\n",
        "## lets use diamonds again but take the numeric columns and standard scale\n",
        "\n",
        "# lets rebuild to practice and show some quick 1-liners\n",
        "# just in case\n",
        "\n",
        "# dia_n = dia.select_dtypes(\"number\")\n",
        "# dia_s = StandardScaler().fit_transform(dia_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdzQ3mef6_oB"
      },
      "source": [
        "# arbitrary selection, but lets keep just 2 components\n",
        "# using svd_solver=full to keep solutions consistent\n",
        "# scikit tries to be smart with auto, but I want to the same solver for examples\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_rJ_2DD_a5Q"
      },
      "source": [
        "# variance explained in just 2?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLQSGsKh71yb"
      },
      "source": [
        "# I will violate my rule and put back onto the original just for exploration\n",
        "# just to explore the fits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNjQbr1_73iV"
      },
      "source": [
        "# remember viz is a use case of dimensionality reduction?\n",
        "# https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\n",
        "\n",
        "# overlay the category\n",
        "# diamonds['cut2'] = diamonds.cut.astype('category').cat.codes\n",
        "# plt.scatter(x=diamonds.pc2_1, y=diamonds.pc2_2, c=diamonds.cut2, cmap=cm.Paired, alpha=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is816RAE9YPM"
      },
      "source": [
        "# lets fit another PCA model, but keep 90% variance\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IVPT2OX9zhp"
      },
      "source": [
        "# Violate again but putting back onto same dataset, but see results\n",
        "\n",
        "# diamonds[['pc90_1', 'pc90_2', 'pc90_3']] = dia_pc90"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W43q9Uc1AH3B"
      },
      "source": [
        "# now lets look at everything\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1rK9cXTogHk"
      },
      "source": [
        "###########################\n",
        "## Summary\n",
        "##\n",
        "## goal is to reduce features and keep the core information\n",
        "## we CAN fit the model to the full dataset\n",
        "## if you want to subset, you can do up front, but we saw how to evaluate when we may not know what to select\n",
        "##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzVs-kplAKOq"
      },
      "source": [
        "##################################################################\n",
        "##\n",
        "##   PCA Compression\n",
        "## \n",
        "## once we have fit our model, we have seen that transform generaâ€ es the newly constructed features\n",
        "## but we can also project back into the original feature space"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5Zbryw5BVP7"
      },
      "source": [
        "# quick refresher, what was the shape that we used?\n",
        "# scaled diamonds numeric without price\n",
        "\n",
        "# dia_s.shape\n",
        "# dia_pc90.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UlKFfC5OsEI"
      },
      "source": [
        "# lets use the fit where we specified 90% to fit\n",
        "\n",
        "# we can use inverse transform to do this\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GInEbt03PG0l"
      },
      "source": [
        "# put back into a dataframe\n",
        "# remember, I scaled dia_n -> dia_s\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiWA5-DXPdVV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKBS2wP0Ppaz"
      },
      "source": [
        "# remember the MNIST dataset I showed on slides\n",
        "# \n",
        "# I showed the aspect of compression there, a simple link\n",
        "# the digit is reconstructed after fitting N components and .inverse_tranform\n",
        "# depends on our use case of course, but highlights that \"losing\" information may not hurt your applications\n",
        "# depending on the dataset, our problem, and our method, we can potentially remove noise and only retain signal!\n",
        "\n",
        "# https://snipboard.io/qrohR7.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "15HOwPsDbwhL"
      },
      "source": [
        "######################################################\n",
        "## ANOTHER APPROACH: tsne\n",
        "## \n",
        "## preserve local structures (distance/neighborhoods) by taking high dimensional data and \n",
        "## representing that structure in a 2d space\n",
        "## NON-LINEAR APPROACH\n",
        "## \n",
        "## can be used as a way to visualize a highly dimensional dataset\n",
        "## but we can think of the new dimensions as features/embeddings of the larger space\n",
        "## \n",
        "##\n",
        "## Let's see this interactively!\n",
        "## https://distill.pub/2016/misread-tsne/\n",
        "##\n",
        "## NOTE: it can be slow for \"larger\" datasets, so be careful\n",
        "## \n",
        "## one common pattern to help with (not solve) is to use PCA to get new features, and then apply TSNE\n",
        "## these techniques highlight how we can put use all of the tools in our toolbox!\n",
        "##\n",
        "## Lets use tsne to show how UML and SML can come together!\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkatSrsZiLgK"
      },
      "source": [
        "# load the digits dataset\n",
        "\n",
        "# from sklearn.datasets import load_digits\n",
        "# digits = load_digits()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-j32XLdiXUl"
      },
      "source": [
        "# extract the data for tsne\n",
        "# we will use X for the \"fit\", y for profiling the output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FpTxVH185IR"
      },
      "source": [
        "# we extracted the data, but we also have images\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9_1XRrli56h"
      },
      "source": [
        "# lets plot it up\n",
        "\n",
        "# plt.imshow(img, cmap=\"gray\")\n",
        "# plt.title(f\"Label: {y[0]}\")\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rX2SkRC58rBd"
      },
      "source": [
        "# do you remember I talked about flattened datasets for images\n",
        "# 1 \"row\" per image, and the pixels as features?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKB1ppBS-A1s"
      },
      "source": [
        "# QUICK THOUGHT EXPERIMENT:\n",
        "# we just saw how easy it is to take a 1 channel image and put it in a shape for machine learning@!\n",
        "# however, what do you think the tradeoff might be as opposed to the natural 8x8 pixels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doUIVho3ARpz"
      },
      "source": [
        "# lets fit a simple model to see how well we can classify the model before tsne\n",
        "\n",
        "# from sklearn.tree import DecisionTreeClassifier  # simple decision tree\n",
        "# tree = DecisionTreeClassifier(max_depth=4)   # max depth of tree of 4 is random for example\n",
        "# tree.fit(X, y)  # sklearn syntax is everywhere!\n",
        "# tree_preds = tree.predict(X)   # \n",
        "# tree_acc = tree.score(X, y)\n",
        "# tree_acc\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YY7YcqVrjqOl"
      },
      "source": [
        "# ^^^ this is our baseline\n",
        "\n",
        "# 64 isn't that bad, but lets use PCA -> tsne which is generally the flow for \"real\" or \"wide\" datasets\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4tSrau-AAet"
      },
      "source": [
        "# two step process - use PCA to reduce, then tsne for embeddings (2d)\n",
        "# to keep things simple, just random choice for 90%, this could be 80%, or 95%\n",
        "\n",
        "# pca_m = PCA(.9)\n",
        "# pca_m.fit(X)\n",
        "# pcs_m = pca_m.transform(X)\n",
        "\n",
        "# # proof\n",
        "# np.sum(pca_m.explained_variance_ratio_)\n",
        "\n",
        "# # how many components did we keep?\n",
        "# pca_m.n_components_\n",
        "\n",
        "# # check this a different way\n",
        "# pcs_m.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0ItYNvbDHOS"
      },
      "source": [
        "# Remember I said TSNE can be slow?\n",
        "# step 2, tsne -- takes a few moments, even with the modest dataset that we are using\n",
        "\n",
        "# tsne = TSNE()\n",
        "# tsne.fit(pcs_m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4ddR-QnEYun"
      },
      "source": [
        "# get the embeddings\n",
        "\n",
        "\n",
        "#\n",
        "# the shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RV-W9OaFxlM"
      },
      "source": [
        "# we know that one of the aims is that tsne helps re-map our data and find the structure\n",
        "# 2d tsne\n",
        "\n",
        "# tdata = pd.DataFrame(te, columns=[\"e1\", \"e2\"])\n",
        "# tdata['y'] = y\n",
        "\n",
        "# tdata.head(3)\n",
        "\n",
        "# tdata.y.value_counts(sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWax-q4WF4Wn"
      },
      "source": [
        "# the plot\n",
        "\n",
        "# PAL = sns.color_palette(\"bright\", 10) \n",
        "# plt.figure(figsize=(10, 8))\n",
        "# sns.scatterplot(x=\"e1\", y=\"e2\", hue=\"y\", data=tdata, legend=\"full\", palette=PAL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFfXMhuoIVxH"
      },
      "source": [
        "# and finally, lets see if we improve the tree now?\n",
        "\n",
        "# X2 = tdata.drop(columns=\"y\")\n",
        "# y2 = tdata.y\n",
        "\n",
        "# tree2 = DecisionTreeClassifier(max_depth=4)\n",
        "# tree2.fit(X2, y2)\n",
        "# tree2_preds = tree2.predict(X2)\n",
        "# tree2_acc2 = tree2.score(X2, y2)\n",
        "# tree2_acc2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shkKGXUUIXv-"
      },
      "source": [
        "# what was the first tree again?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzIJqrtNQC4U"
      },
      "source": [
        "################### YOUR TURN: TSNE Quick hands on practice!\n",
        "## keep just the numeric columns from diamonds\n",
        "## take a random sample of 1000 records\n",
        "## exclude price!\n",
        "## compress the features to 2d   <------- remember to be patient, tsne isn't \"fast\"\n",
        "## visualize the embeddings with price as an overlay\n",
        "## BONUS?  can you fit a linear regression in scikit to predict\n",
        "##         price from the new 2d dimensions?\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7A00xHwrIYou"
      },
      "source": [
        "#####################################   TSNE Review\n",
        "## \n",
        "## what did we just see?\n",
        "## \n",
        "## we saw it took a few moments, thats usually the complaint\n",
        "## one recommendation is to do PCA, but we even saw that it still wasn't immediate on a relatively small dataset\n",
        "## however, even with the base settings, we got real good separation\n",
        "## and a good jump in accuracy!\n",
        "## \n",
        "## almost always 2 components (max 3, but I have only seen two in practice)\n",
        "## \n",
        "## the parameters can be tweaked, and that is worth considering!\n",
        "## perplexity: between 5 and 50, review the resource to see how this can arrive at different solutions\n",
        "## number of iterations: docs suggest at least 250, 1k by default\n",
        "## \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02FQkis-QBqE"
      },
      "source": [
        "############################## UMAP\n",
        "## \n",
        "## another non-linear approach\n",
        "## some research suggests that UMAP can better preserver the \"relationships\" \n",
        "## when going from high dimensional - lower dimensions\n",
        "## \n",
        "## newer approach (2018)\n",
        "## \n",
        "## some research suggests better fit (global structure retention), but its also much faster\n",
        "## reduces the need for pre-processing steps like PCA\n",
        "## can fit more dimensions than TSNE,\n",
        "##\n",
        "##\n",
        "## https://pair-code.github.io/understanding-umap/\n",
        "## \n",
        "## \n",
        "## params to consider\n",
        "##  neighbors (most important)\n",
        "##  random_state (to help with reproducibility)\n",
        "##  number of components (can b2 more than 2, but remember the goal is we want to reduce our feature space!)\n",
        "##  min_dist = review the tools in resources to see how different datasets form different solutions with changing parameters\n",
        "##       - distance in low dimensional space\n",
        "##       - smaller values tend to create tighltly compact embeddings, larger values are \"looser\" \n",
        "##  metric = not heavily discussed, but we can also use different distance \n",
        "##\n",
        "##  ^^ Distance is everywhere in ML!\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fItzFXOUbSTU"
      },
      "source": [
        "# load umap - violating my rule of thumb\n",
        "\n",
        "# from umap import UMAP"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QgcqjMFwbSNR"
      },
      "source": [
        "# rebuild the digits \n",
        "\n",
        "# from sklearn.datasets import load_digits\n",
        "# digits = load_digits()\n",
        "\n",
        "# X = digits.data\n",
        "# y = digits.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I5Jqo56mbSJs"
      },
      "source": [
        "# docs, note the args/kwargs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xfUuhbPbSB8"
      },
      "source": [
        "# can start to type, will find params\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9-NxJT5yjkH"
      },
      "source": [
        "# type\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rNWo6Rmyl__"
      },
      "source": [
        "# shape\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_OexZtQyzyp"
      },
      "source": [
        "# lets put this back into a dataframe\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbY0GBbhzEzW"
      },
      "source": [
        "# what do we have for a scatterplot?  overlay the label \n",
        "sns.scatterplot(x=\"x\", y=\"y\", hue=\"label\", data=umap_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRsqT6LIzVSz"
      },
      "source": [
        "################################## Short(er) Breakout Challenge\n",
        "## use diamonds dataset, take a sample of 5000 rows \n",
        "## keep just the numeric columns, all numeric including price\n",
        "## use tsne and umap to generate 2 new dimensions\n",
        "## create vis to compare\n",
        "## create two kmeans = 5 cluster solutions (think: proxy for cut?)\n",
        "## use tsne embeddings for cluster\n",
        "## use umap for cluster\n",
        "## which has a better silhouette score?\n",
        "## overlay the cut column onto the clusters - profile each cluster by cut, is there agreement?\n",
        "# cluster assignment agreement?  Simply, did the two approaches tend to find similar clusters?\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxqEup4DA8Pp"
      },
      "source": [
        "################################### Breakout Challenge\n",
        "## work as a group to combine UML and SML!\n",
        "## housing-prices tables on Big Query will be used questrom.datasets._______\n",
        "##     housing-train = your training set\n",
        "##     housing-test = the test set, does not have the target, BUT does have an ID that you will need for your submission\n",
        "##     housing-sample-submission = a sample of what a submission file should look like, note the id and your predicted value\n",
        "## \n",
        "## use regression to predict median_house_value\n",
        "## \n",
        "## you can use all of the techniques covered in the program, and this course\n",
        "## objective:  MAE - https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_absolute_error.html\n",
        "##\n",
        "##\n",
        "## tips/tricks\n",
        "##    - ITERATE!  iteration is natural, and your friend\n",
        "##    - submit multiple times with the same team name\n",
        "##    - what would you guess without a model, start there!\n",
        "##    - you will need to submit for all IDs in the test file\n",
        "##    - it will error on submission if you don't submit \n",
        "##\n",
        "## Leaderboard and submissions here: http://34.86.144.106:8501/\n",
        "## "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wi8O5d00riJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eFbpCd_0rlL"
      },
      "source": [
        "################################################# Other approaches to consider!\n",
        "##  MDS - addded as bonus to this session folder\n",
        "## \n",
        "##  Factor Analysis\n",
        "##     -a method to identify latent constructs\n",
        "##     - Techniques that can be applied to market research, \n",
        "##        product management and consumer insights\n",
        "##     - similar to PCA but we don't care about info explained, moreso on the proper construction\n",
        "##       of the constructs that we use for summarization\n",
        "##\n",
        "##  LDA - Linear Discriminant Analysis\n",
        "##     - Like PCA, but we give it a label to help guide the features during reduction\n",
        "##     - https://machinelearningmastery.com/linear-discriminant-analysis-for-dimensionality-reduction-in-python/#:~:text=Linear%20Discriminant%20Analysis%2C%20or%20LDA,examples%20by%20their%20assigned%20class.\n",
        "## \n",
        "##  Other variances of PCA even\n",
        "##     - Randomized PCA\n",
        "##     - Sparse PCA\n",
        "##\n",
        "##  Recommendation Engines\n",
        "##      - extend \"offline\" association rules \n",
        "##      - added some links to the resources (great article with other libraries)\n",
        "##      - toolkits exist to configure good approaches for real-use\n",
        "##      - I call reco engines unsupervised because its moreso about using neighbors and similarity to back \n",
        "##        into items to recommend\n",
        "##      - can be done by finding similar users, or similar items.\n",
        "##      - hybrid approaches work too\n",
        "##      - scikit surprise\n",
        "##      NOTE:  Think about it? you can pull data from databases! you saw flask APIs!  Build your own reco tool!\n",
        "##             batch calculate recos and store in a table, send user id to API, look up the previously made recommendations\n",
        "##             post feedback to database, evaluate, iterate, repeat!\n",
        "##     \n",
        "##   A python package to review\n",
        "##      - I followed this package in its early days (graphlab) before Apple bought the company\n",
        "##      - expressive, with pandas-like syntax\n",
        "##      - accessible toolkit for a number of ML tasks, including Reco engines \n",
        "##      - https://github.com/apple/turicreate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq5ox5WB0rni"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxML0gO20rqC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}