# -*- coding: utf-8 -*-
"""03-clustering2

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lrUf2oiA0VFWWWpDmdm7z9kGXmMXxvpw
"""

## Learning goals:

## Expand on Distance and now apply Kmeans
## - Kmeans applications
## - Evaluate cluster solutions 
## - hands on with Kmeans and other approaches

# installs

# notebook/colab
# ! pip install scikit-plot

# local/server
# pip install scikit-plot

#imports

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# what we need for today
from sklearn.cluster import KMeans, DBSCAN
from sklearn.neighbors import NearestNeighbors
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.cluster.hierarchy import fcluster

from sklearn import metrics 
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

import scikitplot as skplt

# warmup exercise

# dataset:
# https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Election08.csv

# task
# use hierarchical clustering on the election dataset
# keep just the numerical columns
# use complete linkage and generate 4 clusters
# put back onto the original dataset
# profile the number of states by cluster assignment and the % that Obama won

###################
###################
# KMEANS overview

# if on colab
# connect to Big Query

# COLAB Only
# from google.colab import auth
# auth.authenticate_user()
# print('Authenticated')

# lets bring in the judges dataset
# questrom.datasets.judges
# PROJECT = 'questrom'    # <------ change to your project


# judges = pd.read_gbq(SQL, PROJECT)
  
# # make a copy 
# j = judges.copy()

# quick check

# first few rows

# info

# just a preference, but lower case and move judge to the index
# j.columns = j.columns.str.lower()

# j.index = j.judge
# del j['judge']

# confirm we things look good

# QUICK THOUGHT EXERCISE:
# summarize the values
# we talked about scaling the data.  is that needed here?

# scikit learn sytnax
# https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf

# 1. instantiate an object of the class we need
# 2. apply the object our dataset - can think of this as model fitting even for preprocessing
# 3. fit/apply the model/task to ANY data

# Kmeans via sklearn
# help(KMeans)
# n_init = # of fits with best intertia selected 
#      ^ Sum of squared distances of samples to their closest cluster center.

# instantiate
# k3 = KMeans(3)

# fit to the the data
# k3.fit(j)

## the output is not an error, just info on the config

# we previously used fcluster to get assignments of clusters per row

# get the cluster assignments in sklearn via the predict method

# we didnt save out the assignments
# 0-based cluster "labels" in the same order of the data we passed in

# we can do anything we want with the assignments, including put back onto the data

# judges['k3'] = k3_labs

# quick review

# EXERCISE:  
#           Your turn!  Fit 5 clusters and add it the judges dataset too
#           BONUS: set the number of iterations to 100

# lets inspect k5

# how many iterations were actully run?
# k5.n_iter_

# we can get cluster centers (by feature)
# shape = (# clusters, # features)
# k5.cluster_centers_

# quick reference

# our dataset has more than 2 dimensions, but lets plot this out
# we want to put the cluster centers on top of the first two features

# k5_centers = k5.cluster_centers_
# sns.scatterplot(data=judges, x="CONT", y="INTG", cmap="virdis", hue="k5")
# plt.scatter(k5_centers[:,0], k5_centers[:,1], c="g", s=100)

# remember, our dataset has 12 dimensions, we are trying to force the 
# centers on top of 2 of the 12 via a scatter

# EXERCISE:
#         https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/MedGPA.csv
#         the dataset is at the URL above ^^ 
#         fit 2 clusters [using just the GPA and BS columns]
#         (this is a dataset about admissions)
#         put the clusters back onto the original datset
#         scatterplot of the clusters
#             for the plot, color by the cluster assignment
#             the marker should be the actual admissions decision (variable = Acceptance)

#######################
# OK, 
# lets start to think about cluster evaluation

# we are going to focus on intertia and silhouette scores

# but for more info, you could consider below when we have known labels (that we choose to ignore_
#  Homogeneity and Completeness
#  (cluster only has 1 class) vs (all labels for a class are found in 1 cluster)

# we have fit two cluster solutions to the judges dataset

# this is the total sum of squared error for each sample to it's cluster's centroid
# we want to minimize this value, and 5 suggests we have a good improvement

# Exericse:
#           Take 2 minutes:  Based on what you have know, 
#           how might you think about evaluating a range of cluster solutions based
#           on inertia
#

# let's code this up

# plot it up

# one rule of thumb is the elbow method

# we might gain, but if the rate of improvement slows, we might be better off with "smaller"/easier solution

# logic: if our goal is to create segments, do we really want 1 segment per observation?

# silohouette scores

# each observation gets a score
# we average across for a metric on the overall cluster solution
# we can also plot the fit by cluster/row to evaluate

# for k5 earlier, lets get the silohouette scores
# the original data matrix, and then the labels
# silo_overall = metrics.silhouette_score(j, k5.predict(j))

# silo_overall

# just like inertia, a metric for the solution, and we can compare
# across various K, except we want to maxmize this value

# we can also get it for each observeration

# silo_sample = metrics.silhouette_samples(j, k5.predict(j))
# silo_sample

# and lets plot this up with skplot

# skplt.metrics.plot_silhouette(j, k5.predict(j), figsize=(7,7))

# notes
# figsize set inside skplot


# fit doesnt look too bad, we want positive values, and only not many are

# negative values don't tell us the direction of the solution, just that
# we may have too many or too few clusters

# above is a really good fit, but it doesn't mean it's the best just yet
# though the inertia plot suggested an elbow at 5

# just like kmeans, lets test k

# KRANGE = range(2, 30)

# # containers
# ss = []


# for k in KRANGE:
#   km = KMeans(k)
#   lab = km.fit_predict(j)
#   ss.append(metrics.silhouette_score(j, lab))

# the plot 
# sns.lineplot(KRANGE, ss)

############################

# Takeaways:

# larger number of clusters in both cases decline on both metrics
# but we start to flatten out around 5 as well, a good sign

# what does this mean?

# if our business problem dictates, perhaps 2 is a good solution here, but inconsistent with interia
# if our goal is segment into manageable groups for down stream strategic action
# we need to balance capability to action on clusters, with "theoretical" fit

###################
# DBSCAN
# https://scikit-learn.org/stable/modules/clustering.html#dbscan

# we saw hierarchical approaches
# we just ran through a partitioning approach
# finally, a density approach

# lets make a fake dataset using sklearn

# X, _ = make_blobs(random_state=820)

# put this into a dataframe
# X = pd.DataFrame(X, columns=['a', 'b'])
# sns.scatterplot(data=X, x="a", y="b")

# summarize

# lets look at a way to figure out a rough way to determine episolon (the neighborhood boundary)
# we can use NearestNeighbors to get distances for a closest point

# nb = NearestNeighbors(2)
# nbrs = nb.fit(X)

# # get the distances to closest point and the indices for each row
# distances, indices = nbrs.kneighbors(X)

# # sort the rows
# distances = np.sort(distances, axis=0)

# # we can use the distance
# # the first "column" is 0, or itself
# distances = distances[:,1]


# sns.lineplot(range(len(distances)), distances)

# elbow method put into the setup of the approach
# very similar to what we have seen already, right? (from sklearn)

# db = DBSCAN(eps=.7, min_samples=5)
# dbc = db.fit_predict(X)
# dbc

# lets plot it up
# X['labels'] = dbc

# pal = sns.color_palette("husl", len(np.unique(dbc)))
# plt.figure(figsize=(10,6))
# sns.scatterplot(data=X, x="a", y="b", hue="labels", palette=pal)

# Considerations:
# of course, we could overlay silhouette score

# Thought exercise:
# Think about the fact that we generated fake data (and its summary)
# What might we try or consider to think about finding a "better" solution